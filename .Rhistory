glimpse(pima)
# Multivarijabilna linearna regresija - prostiji model (ukljucen manji broj promenljivih)
fit_1 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_1 <- predict(fit_1)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_1 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_1) ^ 2))
rmse_1
# Koristicemo "pima" bazu
# Struktura seta podataka
str(pima)
# Multivarijabilna linearna regresija - prostiji model (ukljucen manji broj promenljivih)
fit_1 <- lm(diabetes ~ bmi + triceps + age + glucose, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_1 <- predict(fit_1)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_1 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_1) ^ 2))
rmse_1
# Multivarijabilna linearna regresija - kompleksniji model (ukljucen veci broj promenljivih)
fit_2 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic + inslulin + pregnant, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_2 <- predict(fit_2)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_2 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_2) ^ 2))
rmse_2
str(pima)
# Multivarijabilna linearna regresija - prostiji model (ukljucen manji broj promenljivih)
fit_1 <- lm(diabetes ~ bmi + triceps + age + glucose, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_1 <- predict(fit_1)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_1 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_1) ^ 2))
rmse_1
# Multivarijabilna linearna regresija - kompleksniji model (ukljucen veci broj promenljivih)
fit_2 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic + inslulin + pregnant, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_2 <- predict(fit_2)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_2 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_2) ^ 2))
rmse_2
pima
fit_2 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic + inslulin + pregnant, data = pima)
# Koristicemo "pima" bazu
# Struktura seta podataka
str(pima)
# Multivarijabilna linearna regresija - prostiji model (ukljucen manji broj promenljivih)
fit_1 <- lm(diabetes ~ bmi + triceps + age + glucose, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_1 <- predict(fit_1)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_1 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_1) ^ 2))
rmse_1
# Multivarijabilna linearna regresija - kompleksniji model (ukljucen veci broj promenljivih)
fit_2 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic + insulin + pregnant, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_2 <- predict(fit_2)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_2 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_2) ^ 2))
rmse_2
# Da bi smo obezbedili reproduktibilnost
set.seed(1)
# Proveravamo strukturu podataka
str(iris)
head(iris)
# Delimo "iris" na dva seta: "my_iris" i "species""
my_iris <- iris[-5]
species <- iris$Species
# Vrsimo k-means klasterizaciju za "my_iris", pretpostavljamo da postoje tri klase: "kmeans_iris"
kmeans_iris <- kmeans(my_iris,3)
# Poredimo dobijene klastere sa istinskim klasama (kategorijama)
table(species, kmeans_iris$cluster)
# Plotujemo "Petal.Width" vs "Petal.Length", bojimo po klasterima odn. postojecim kategorijama
par(mfrow = c(1,2))
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)
title("k-means - klasteri")
plot(Petal.Length ~ Petal.Width, data = my_iris, col = iris$Species)
title("Istinske klase")
kmeans_iris$tot.withinss/kmeans_iris$betweenss
?sample
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(Survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$Survived, pred)
# Prikaz matrice konfuzije
conf
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
library(rpart)
library(readr)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Da obezbedimo reproduktibilnost
set.seed(33)
# Proveravamo strukturu data seta
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(Survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$Survived, pred)
# Prikaz matrice konfuzije
conf
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Da obezbedimo reproduktibilnost
set.seed(33)
# Proveravamo strukturu data seta
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(Survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
library(rpart)
library(readr)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Da obezbedimo reproduktibilnost
set.seed(33)
# Proveravamo strukturu data seta
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
tree <- rpart(survived ~ ., data = train, method = "class")
pred <- predict(tree, newdata = test, type = "class")
n <- nrow(titanic)
shuffled <- titanic[sample(n),]
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
train
test
test <- shuffled[-train_indicies, ]
pred <- predict(tree, newdata = titanic, type = "class")
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka:
tree <- rpart(survived ~ ., data = titanic, method = "class")
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
pred <- predict(tree, newdata = titanic, type = "class")
pred <- predict(tree, newdata = test, type = "class")
test <- shuffled[-train_indicies, -1]
tree <- rpart(survived ~ ., data = train, method = "class")
pred <- predict(tree, newdata = test, type = "class")
pred
tree <- rpart(survived ~ ., data = titanic, method = "class")
test <- shuffled[-train_indicies, ]
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
tree <- rpart(survived ~ ., data = train, method = "class")
pred <- predict(tree, newdata = train, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
tree <- rpart(survived ~ ., data = train, method = "class")
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
str(titanic)
titanic <- data_frame(titanic)
titanic <- as.data.frame(titanic)
str(titanic)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
View(titanic)
View(titanic)
str(titanic)
titanic <- titanic[, c(1, 2, 4, 5)]
str(titanic)
library(purr)
install.packages("purr")
library(purrr)
map(titanic[-4], as.factor)
titanic[-4] <- map(titanic[-4], as.factor)
str(titanic)
table(titanic$survived)
prop.table(table(titanic$survived))
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka:
tree <- rpart(survived ~ ., data = titanic, method = "class")
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
# Konstruisemo konfuzionu matricu koristeci "table()":
table(titanic$survived, pred)
library(rpart)
library(readr)
library(purrr)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Proveravamo strukturu data seta
str(titanic)
# Koristicemo samo kolone 'survived', 'pclass', 'sex' i 'age'
titanic <- titanic[, c(1, 2, 4, 5)]
str(titanic)
# Prve tri promenlive bi evidentno trebalo da budu tretirane kao kategoricke promenljive - faktori
titanic[-4] <- map(titanic[-4], as.factor)
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
train
tree
library(rpart)
library(readr)
library(purrr)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Proveravamo strukturu data seta
str(titanic)
# Koristicemo samo kolone 'survived', 'pclass', 'sex' i 'age'
titanic <- titanic[, c(1, 2, 4, 5)]
str(titanic)
# Prve tri promenlive bi evidentno trebalo da budu tretirane kao kategoricke promenljive - faktori
titanic[-4] <- map(titanic[-4], as.factor)
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf
accs <- rep(0,6)
accs
str(titanic)
# Da bismo obezbedili reproduktibilnost
set.seed(1)
# Koristicemo prethodno formirani "shuffled" skup podataka
# Inicijalizujemo vektor accs - popunjavamo nulama
accs <- rep(0,9)
# Treniramo model koristeci kros-validacione intervale vrednosti i vrsimo estimaciju tacnosti modela kao prosecne vrednosti tacnosti sracunatih unakrsnom validacijom
for (i in 1:9) {
# Ovi indeksi ukazuju na trenutni interval test seta koji koristimo za treniranje modela
indices <- (((i - 1) * round((1/9)*nrow(shuffled))) + 1):((i*round((1/9) * nrow(shuffled))))
# Iskljucujemo ove intervale iz trening seta
train <- shuffled[-indices,]
# Ukljucimo ih u test set
test <- shuffled[indices,]
# Treniramo model sa svakim od dobijenih trening setova po iteracijama
tree <- rpart(survived ~ ., train, method = "class")
# Predvidjamo klase za tekuci test set u svakoj od iteracija
pred <- predict(tree, test, type = "class")
# Formiramo odgovarajucu konfuzionu matricu
conf <- table(test$Survived, pred)
# Dodeljujemo vrednost za tacnost tekuceg modela i-tom indeksu u vektoru accs
accs[i] <- sum(diag(conf))/sum(conf)
}
# Srednja vrednost za accs
mean(accs)
# Da bismo obezbedili reproduktibilnost
set.seed(33)
# Koristicemo prethodno formirani "shuffled" skup podataka
# Inicijalizujemo vektor accs - popunjavamo nulama
accs <- rep(0,9)
# Treniramo model koristeci kros-validacione intervale vrednosti i vrsimo estimaciju tacnosti modela kao prosecne vrednosti tacnosti sracunatih unakrsnom validacijom
for (i in 1:9) {
# Ovi indeksi ukazuju na trenutni interval test seta koji koristimo za treniranje modela
indices <- (((i - 1) * round((1/9)*nrow(shuffled))) + 1):((i*round((1/9) * nrow(shuffled))))
# Iskljucujemo ove intervale iz trening seta
train <- shuffled[-indices,]
# Ukljucimo ih u test set
test <- shuffled[indices,]
# Treniramo model sa svakim od dobijenih trening setova po iteracijama
tree <- rpart(survived ~ ., train, method = "class")
# Predvidjamo klase za tekuci test set u svakoj od iteracija
pred <- predict(tree, test, type = "class")
# Formiramo odgovarajucu konfuzionu matricu
conf <- table(test$survived, pred)
# Dodeljujemo vrednost za tacnost tekuceg modela i-tom indeksu u vektoru accs
accs[i] <- sum(diag(conf))/sum(conf)
}
# Srednja vrednost za accs
mean(accs)
getwd()
library(readr)
if (!"emails_full" %in% ls()) {
emails_full <- read_csv("data/spambase.data")
}
if (!"emails_full" %in% ls()) {
emails_full <- read.csv("data/spambase.data")
}
str(emails_full)
if (!"emails_full" %in% ls()) {
emails_full <- read_csv("data/spambase.data", header = FALSE)
}
if (!"emails_full" %in% ls()) {
emails_full <- read.csv("data/spambase.data", header = FALSE)
}
str(emails_full)
emails_full <- emails_full[, c(57, 58)]
str(emails_full)
library(readr)
if (!"emails" %in% ls()) {
emails <- read_csv("data/emails_small.csv")
}
# Proveravamo strukturu seta podataka
str(emails)
emails_full <- emails_full[, c(55, 58)]
str(emails_full)
if (!"emails_full" %in% ls()) {
emails_full <- read.csv("data/spambase.data", header = FALSE)
}
# Proveravamo strukturu seta podataka
str(emails_full)
# Na osnovu dokumentacije...
emails_full <- emails_full[, c(55, 58)]
str(emails_full)
library(readr)
if (!"emails" %in% ls()) {
emails <- read_csv("data/emails_small.csv")
}
# Proveravamo strukturu seta podataka
str(emails)
colnames(emails_full) <-  c(" avg_capital_seq", "spam")
str(emails_full)
colnames(emails_full) <-  c("avg_capital_seq", "spam")
emails_full$avg_capital_seq <- as.factor(emails_full$avg_capital_seq)
str(emails_full)
View(emails_full)
rm(emails_full)
if (!"emails_full" %in% ls()) {
emails_full <- read.csv("data/spambase.data", header = FALSE)
}
# Proveravamo strukturu seta podataka
str(emails_full)
# Na osnovu dokumentacije...
emails_full <- emails_full[, c(55, 58)]
str(emails_full)
colnames(emails_full) <-  c("avg_capital_seq", "spam")
str(emails_full)
emails_full$spam <- as.factor(emails_full$spam)
str(emails_full)
# Definisemo funkciju spam_classifier()
# 1 - spam, 0 - ham
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x >= 3 & x <= 4] <- 0
prediction[x >= 2.2 & x < 3] <- 1
prediction[x >= 1.4 & x < 2.2] <- 0
prediction[x > 1.25 & x < 1.4] <- 1
prediction[x <= 1.25] <- 0
return(factor(prediction, levels = c("1","0")))
}
# Primenimo spam_classifier na emails_full: pred_full
pred_full <- spam_classifier(emails_full$avg_capital_seq)
# Konfuziona matrica za emails_full: conf_full
conf_full <- table(emails_full$spam, pred_full)
# Racunamo tacnost na osnovu conf_full: acc_full
acc_full <- sum(diag(conf_full))/sum(conf_full)
acc_full
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x <= 4] <- 0
return(factor(prediction, levels=c("1","0")))
}
# conf_small and acc_small have been calculated for you
conf_small <- table(emails_small$spam, spam_classifier(emails_small$avg_capital_seq))
acc_small <- sum(diag(conf_small)) / sum(conf_small)
acc_small
# Apply spam_classifier to emails_full and calculate the confusion matrix: conf_full
conf_full <- table(emails_full$spam, spam_classifier(emails_full$avg_capital_seq))
# Calculate acc_full
acc_full <- sum(diag(conf_full)) / sum(conf_full)
# Print acc_full
acc_full
str(emails_full)
# Definisemo funkciju spam_classifier()
# 1 - spam, 0 - ham
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x >= 3 & x <= 4] <- 0
prediction[x >= 2.2 & x < 3] <- 1
prediction[x >= 1.4 & x < 2.2] <- 0
prediction[x > 1.25 & x < 1.4] <- 1
prediction[x <= 1.25] <- 0
return(factor(prediction, levels = c("0","1")))
}
# Primenimo spam_classifier na emails_full: pred_full
pred_full <- spam_classifier(emails_full$avg_capital_seq)
# Konfuziona matrica za emails_full: conf_full
conf_full <- table(emails_full$spam, pred_full)
# Racunamo tacnost na osnovu conf_full: acc_full
acc_full <- sum(diag(conf_full))/sum(conf_full)
acc_full
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x <= 4] <- 0
return(factor(prediction, levels = c("0","1")))
}
#
conf_small <- table(emails$spam, spam_classifier(emails$avg_capital_seq))
acc_small <- sum(diag(conf_small)) / sum(conf_small)
acc_small
# Apply spam_classifier to emails_full and calculate the confusion matrix: conf_full
conf_full <- table(emails_full$spam, spam_classifier(emails_full$avg_capital_seq))
# Calculate acc_full
acc_full <- sum(diag(conf_full)) / sum(conf_full)
acc_full
