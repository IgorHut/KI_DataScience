# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka, tj. vrsimo trening naseg modela:
tree <- rpart(survived ~ ., data = train, method = "class")
# Hajde da vidimo kako nase stablo odlucivanja izgleda
fancyRpartPlot(tree)
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
str(train)
str(test)
if (!"test" %in% ls()) {
test <- read_csv("data/test.csv")
}
str(test)
# Uvoz podataka za "train" i "test" set
if (!"train" %in% ls()) {
train <- read_csv("data/train.csv")
}
str(train)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
# broj promenljivih i broj opservacija
str(iris)
dim(iris)
# Nekoliko prvih i poslednjih vrsta iz `iris` baze
head(iris)
tail(iris)
# sumarna statistika za podatke u `iris` bazi
summary(iris)
plot(iris) #rezultat ce biti isti kao da smo upotrebili `pairs` funkciju iz `base` bilioteke
ggplot(iris, aes( x = Sepal.Width, y = Sepal.Length, col = Species)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
# Ucitavamo "Wage" bazu iz "ISLR" paketa koja sadrzi neke opste podatke o radnicima u srednje-Atlanstkom regionu SAD i njihovim prihodima
library(ISLR)
data("Wage")
# Generisemo linearni model "lm_wage"
lm_wage <- lm(wage ~ age, data = Wage)
# ?lm - kako se funkcija "lm()" koristi
# Definisemo data.frame sa novim vrednostima, koje nisu koriscenje za sintezu modela: "unseen"
unseen <- data.frame(age = 60)
# Na osnovu modela "lm_wage" predvidjamo koliko iznosi plata 60-togodisnjeg radnika
predict(lm_wage, unseen)
# Broj pregleda vased LinkedIn profila u periodu od tri nedelje
linkedin <- c(5, 7,  4,  9, 11, 10, 14, 17, 13, 11, 18, 17, 21, 21, 24, 23, 28, 35, 21, 27, 23)
# Vektor koji sadrzi korespodentne dane: "dani"
dani <- 1:21
# Linearni model - broj pregleda po danima: linkedin_lm
linkedin_lm <- lm(linkedin ~ dani)
# Predvidjamo broj pregleda u sledeca tri dana: linkedin_pred
buduci_dani <- data.frame(dani = 22:24)
linkedin_pred <- predict(linkedin_lm, buduci_dani)
# Plotujemo "istorijske" podatke i predvidjanje
plot(linkedin ~ dani, xlim = c(1, 24))
points(22:24, linkedin_pred, col = "red", pch = 16)
library(readr)
if (!"emails" %in% ls()) {
emails <- read_csv("data/emails_small.csv")
}
# Proveravamo strukturu seta podataka
str(emails)
# Definisemo funkciju spam_classifier()
# 1 - spam, 0 - ham
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x >= 3 & x <= 4] <- 0
prediction[x >= 2.2 & x < 3] <- 1
prediction[x >= 1.4 & x < 2.2] <- 0
prediction[x > 1.25 & x < 1.4] <- 1
prediction[x <= 1.25] <- 0
return(prediction)
}
# Primenimo nas klasifikator na kolonu "avg_capital_seq": "spam_pred"
spam_pred <- spam_classifier(emails$avg_capital_seq)
# Uporedimo "spam_pred" i  "emails$spam"
spam_pred == emails$spam
identical(spam_pred, as.numeric(emails$spam))
# Da bi smo obezbedili reproduktibilnost
set.seed(1)
# Proveravamo strukturu podataka
str(iris)
head(iris)
# Delimo "iris" na dva seta: "my_iris" i "species""
my_iris <- iris[-5]
species <- iris$Species
# Vrsimo k-means klasterizaciju za "my_iris", pretpostavljamo da postoje tri klase: "kmeans_iris"
kmeans_iris <- kmeans(my_iris,3)
# Poredimo dobijene klastere sa istinskim klasama (kategorijama)
table(species, kmeans_iris$cluster)
# Plotujemo "Petal.Width" vs "Petal.Length", bojimo po klasterima odn. postojecim kategorijama
par(mfrow = c(1,2))
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)
title("k-means - klasteri")
plot(Petal.Length ~ Petal.Width, data = my_iris, col = iris$Species)
title("Istinske klase")
library(rpart)
library(readr)
library(purrr)
# Import podataka
if (!"titanic" %in% ls()) {
titanic <- read_csv("data/train.csv")
}
# Da obezbedimo reproduktibilnost
set.seed(33)
# Proveravamo strukturu data seta
str(titanic)
# Koristicemo samo kolone 'survived', 'pclass', 'sex' i 'age'
titanic <- titanic[, c(1, 2, 4, 5)]
str(titanic)
# Prve tri promenlive bi evidentno trebalo da budu tretirane kao kategoricke promenljive - faktori
titanic[-4] <- map(titanic[-4], as.factor)
str(titanic)
table(titanic$survived)
# Odnos broja prezivelih i poginulih
prop.table(table(titanic$survived))
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka:
tree <- rpart(survived ~ ., data = titanic, method = "class")
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
# Konstruisemo konfuzionu matricu koristeci "table()":
conf_t <- table(titanic$survived, pred)
conf_t
#Isto to sa "pima" bazom podataka
library(faraway)
data(pima)
head(pima)
str(pima)
# Da bismo obezbedili reproduktibilnost
set.seed(33)
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka:
tree <- rpart(test ~ ., data = pima, method = "class")
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = pima, type = "class")
# Konstruisemo konfuzionu matricu koristeci "table()":
conf_p <- table(pima$test, pred)
conf_p
# Izracunajmo parametre za ocenu valjanosti modela "tree" za "titanic" skup podataka
# Formiramo TP, FN, FP i TN na osnovu "conf_t"
TP <- conf_t[2,2]
FP <- conf_t[1,2]
FN <- conf_t[2,1]
TN <- conf_t[1,1]
# Tacnost (Accuracy)
acc <- (TP + TN)/sum(conf_t)
acc
# Preciznost (Precision)
prec <- TP/(TP + FP)
prec
# Senzitivnost (Sensitivity, Recall)
sens <- TP/(TP + FN)
sens
# Specificnost (Specificity)
spec <- TN/(TN + FP)
spec
# Koristicemo "pima" bazu
# Struktura seta podataka
str(pima)
# Multivarijabilna linearna regresija - prostiji model (ukljucen manji broj promenljivih)
fit_1 <- lm(diabetes ~ bmi + triceps + age + glucose, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_1 <- predict(fit_1)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_1 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_1) ^ 2))
rmse_1
# Multivarijabilna linearna regresija - kompleksniji model (ukljucen veci broj promenljivih)
fit_2 <- lm(diabetes ~ bmi + triceps + age + glucose + diastolic + insulin + pregnant, data = pima)
# Predvidjanje na osnovu modela: pred_1
pred_2 <- predict(fit_2)
# RMSE na osnovu "pima$diabetes" (tacne vrednosti) i "pred_1" (vrednosti na osnovu modela fit_1)
rmse_2 <- sqrt(1/nrow(pima)*sum((pima$diabetes - pred_2) ^ 2))
rmse_2
# Da bi smo obezbedili reproduktibilnost
set.seed(33)
# Proveravamo strukturu podataka
str(iris)
head(iris)
# Delimo "iris" na dva seta: "my_iris" i "species""
my_iris <- iris[-5]
species <- iris$Species
# Vrsimo k-means klasterizaciju za "my_iris" uz pretpostavku da postoje tri klase: "kmeans_iris"
kmeans_iris <- kmeans(my_iris,3)
# Poredimo dobijene klastere sa istinskim klasama (kategorijama)
table(species, kmeans_iris$cluster)
# Plotujemo "Petal.Width" vs "Petal.Length", bojimo po klasterima odn. postojecim kategorijama
par(mfrow = c(1,2))
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)
title("k-means - klasteri")
plot(Petal.Length ~ Petal.Width, data = my_iris, col = iris$Species)
title("Istinske klase")
kmeans_iris$tot.withinss/kmeans_iris$betweenss
# Koristicemo "titanic" set podataka formiran u jednom od prethodnih primera
str(titanic)
table(titanic$survived)
# Odnos prezivelih i poginulih
prop.table(table(titanic$survived))
# Da bismo omogucili reproduktibilnost
set.seed(33)
# Prvo napravimo jednu slucajno odabranu permutaciju celog skupa podataka (dataset shuffle)
n <- nrow(titanic)
shuffled <- titanic[sample(n),] #f-a 'sample' vrsi slucajno odabiranje elemenata zadatog vektora
# Delimo skup podataka na trening i test set (70% i 30%)
train_indicies <- 1:round(0.7 * n)
train <- shuffled[train_indicies, ]
test <- shuffled[-train_indicies, ]
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu trening seta:
tree <- rpart(survived ~ ., data = train, method = "class")
# Koristeci dobijeni model "tree" vrsimo klasifikaciju podataka iz test seta:
pred <- predict(tree, newdata = test, type = "class")
# Racunamo matricu konfuzije
conf_t <- table(test$survived, pred)
# Prikaz matrice konfuzije
conf_t
# Formiramo TP, FN, FP i TN na osnovu "conf_t"
TP <- conf_t[2,2]
FP <- conf_t[1,2]
FN <- conf_t[2,1]
TN <- conf_t[1,1]
# Tacnost (Accuracy)
acc <- (TP + TN)/sum(conf_t)
acc
# Preciznost (Precision)
prec <- TP/(TP + FP)
prec
# Senzitivnost (Sensitivity, Recall)
sens <- TP/(TP + FN)
sens
# Specificnost (Specificity)
spec <- TN/(TN + FP)
spec
# Da bismo obezbedili reproduktibilnost
set.seed(33)
# Koristicemo prethodno formirani "shuffled" skup podataka
# Inicijalizujemo vektor accs - popunjavamo nulama
accs <- rep(0,9)
# Treniramo model koristeci kros-validacione intervale vrednosti i vrsimo estimaciju tacnosti modela kao prosecne vrednost tacnosti sracunate unakrsnom validacijom
for (i in 1:9) {
# Ovi indeksi ukazuju na trenutni interval test seta koji koristimo za treniranje modela
indices <- (((i - 1) * round((1/9)*nrow(shuffled))) + 1):((i*round((1/9) * nrow(shuffled))))
# Iskljucujemo ove intervale iz trening seta
train <- shuffled[-indices,]
# Ukljucimo ih u test set
test <- shuffled[indices,]
# Treniramo model sa svakim od dobijenih trening setova po iteracijama
tree <- rpart(survived ~ ., train, method = "class")
# Predvidjamo klase za tekuci test set u svakoj od iteracija
pred <- predict(tree, test, type = "class")
# Formiramo odgovarajucu konfuzionu matricu
conf <- table(test$survived, pred)
# Dodeljujemo vrednost za tacnost tekuceg modela i-tom indeksu u vektoru accs
accs[i] <- sum(diag(conf))/sum(conf)
}
# Srednja vrednost za accs
mean(accs)
if (!"emails_full" %in% ls()) {
emails_full <- read.csv("data/spambase.data", header = FALSE)
}
# Proveravamo strukturu seta podataka
str(emails_full)
# Na osnovu dokumentacije...
emails_full <- emails_full[, c(55, 58)]
str(emails_full)
colnames(emails_full) <-  c("avg_capital_seq", "spam")
str(emails_full)
emails_full$spam <- as.factor(emails_full$spam)
str(emails_full)
# Definisemo funkciju spam_classifier()
# 1 - spam, 0 - ham
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x >= 3 & x <= 4] <- 0
prediction[x >= 2.2 & x < 3] <- 1
prediction[x >= 1.4 & x < 2.2] <- 0
prediction[x > 1.25 & x < 1.4] <- 1
prediction[x <= 1.25] <- 0
return(factor(prediction, levels = c("0","1")))
}
# Primenimo spam_classifier na emails_full: pred_full
pred_full <- spam_classifier(emails_full$avg_capital_seq)
# Konfuziona matrica za emails_full: conf_full
conf_full <- table(emails_full$spam, pred_full)
# Racunamo tacnost na osnovu conf_full: acc_full
acc_full <- sum(diag(conf_full))/sum(conf_full)
acc_full
# Uproscen model za klasifikaciju
spam_classifier <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 4] <- 1
prediction[x <= 4] <- 0
return(factor(prediction, levels = c("0","1")))
}
# Tacnost predikcije sa uproscenim modelom za emails data set
conf_small <- table(emails$spam, spam_classifier(emails$avg_capital_seq))
acc_small <- sum(diag(conf_small)) / sum(conf_small)
acc_small
# Primenimo uprosceni model i na "emails_full" i sracunamo konfuzionu matricu
conf_full <- table(emails_full$spam, spam_classifier(emails_full$avg_capital_seq))
# Izracunamo tacnost
acc_full <- sum(diag(conf_full)) / sum(conf_full)
acc_full
library(MASS)
library(ISLR)
library(ggplot2)
?Boston
str(Boston)
head(Boston)
# Proverimo kako izgleda promena "medv" (median value of owner-occupied homes in \$1000s) sa
# "lstat" (lower status of the population (percent)
plot(medv~lstat,Boston)
# Kao sto vidimo postoji jasan trend opadanja vrednosti nekretnina sa porastom procenta
# siromasnijih stanovnika (ovakva korelacija je naravno i ocekivana). Ovakvi slucajevi su dobri
# kandidati za modelovanje prostom linerarnom regresijom.
fit_1 = lm(medv ~ lstat, data = Boston)
# Hajde da vidimo kako izgleda nas model
fit_1
# Detaljniji uvid
summary(fit_1)
# Sta sve model sadrzi
names(fit_1)
# Samo koeficijenti
fit_1$coefficients
# Ucrtajmo regresionu pravu na pocetni scatter plot
abline(fit_1$coefficients, col = "red", lwd = 2)
# Ili sve zajedno koristerci "ggplot2"
ggplot(Boston, aes( x = lstat, y = medv)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, colour = "red")
# Ako zelimo i interval pouzdanosti sam izostavimo parametar "se" (podrazumevano se = TRUE)
ggplot(Boston, aes( x = lstat, y = medv)) +
geom_point() +
geom_smooth(method = "lm")
# Interval pouzdanosti
confint(fit_1)
# Da predvidimo vrednosti "medv" za dati vektor vrednosti "lstat" promenljive, uz
# proracun intervala pouzdanosti
predict(fit_1,data.frame(lstat = c(5,10,15)),interval = "confidence")
library(readr)
library(tidyr)
library(purrr)
library(ggpubr)
# Uvoz i sredjivanje podataka
shop_data <- read_csv("data/shop_data.csv")
shop_data <- separate(shop_data, '"sales","sq_ft","inv","ads","size_dist","comp"',
c("sales","sq_ft","inv","ads","size_dist","comp"), sep = ",")
shop_data <- as.data.frame(map(shop_data, as.numeric))
str(shop_data)
head(shop_data)
# Hajde da proverimo kako se podaci ponasaju i mogu li se uociti relacije
# izmedju distribucija promenljivih koje bi ukazivale na opravdanost uvodjenja
# linearnog modela:
par(mfrow = c(2,3))
plot(sales ~ ads, shop_data)
plot(sales ~ sq_ft, shop_data)
plot(sales ~ size_dist, shop_data)
plot(sales ~ inv, shop_data)
plot(sales ~ comp, shop_data)
#Ili:
pairs(shop_data)
# Linearni model za "sales" koji ukjucuje sve prediktore (sve preostale promenljive)
lm_shop_1 <- lm( sales ~., data = shop_data)
# Proverimo parametre valjanosti modela i koliko su pojedini prediktori znacajni u modelu:
summary(lm_shop_1)
# Da bismo uopste mogli da koristimo p-vrednosti u ovom kontekstu treba prvo da proverimo
# da li je zadovoljena pretpostavka o normalnoj distribuciji reziduala!
par(mfrow = c(1,2))
# Plotujemo reziduale u funkciji fitovanih vrednosti za pojedinacje opservacije
plot(lm_shop_1$fitted.values, lm_shop_1$residuals)
abline(0,0, col = "red", lty = 2)
# Napravimo  Q-Q plot kvantila reziduala
qqnorm(lm_shop_1$residuals, ylab = "Residual Quantiles")
qqline(lm_shop_1$residuals, col = "red")
par(mfrow = c(1,1))
# Mozemo i da upotrebimo f-ju "ggqqplot" iz paketa "ggpubr" koji sadrzi funkcije za
# plotovanje "lepih" grafika:
ggqqplot(lm_shop_1$residuals, ylab = "Residual Quantiles")
# Me moze se uociti nikakav jasan "pattern" u distribucij reziduala, sta vise kvantili
# reziduala su uglavnom na liniji koja odgovara teorijskoj - normalnoj distribuciji
# Proverimo ponovo summary
summary(lm_shop_1)
# Iskoristimo sada dobijeni model da predvidimo neto prodajnu vrednost na osnovu novog
# skupa prediktora:
shop_new = data.frame("sq_ft" = 2.3, "inv" = 420, "ads" = 8.7,
"size_dist" = 9.1, "comp" = 10)
predict(lm_shop_1, shop_new)
# Linearni model za "medv" na osnovu dva prediktora: "lstat" i "age"
fit2 = lm(medv ~ lstat + age, data = Boston)
summary(fit2)
#Linearni model za "medv" na osnovu svih raspolozivih prediktora
fit3 = lm(medv ~.,Boston)
summary(fit3)
# Jos jedan nacin da se iscraju grafici koji se koriste za procenu valjanosti i opravdanosti linearnog modela
par(mfrow = c(2,2))
plot(fit3)
# Na osnovu "summary" za model fit3 videli smo da promenljive "indus" i "age" ne igraju bitnu ulogu, te cemo update-vati model tako da ih ne uzima u obzir:
fit4 = update(fit3,~.-age-indus)
summary(fit4)
# Ucitajmo prvo pakete koji ce biti korisceni u ovom primeru
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Da se podsetimo kako izgleda skup podataka koji cemo koristiti u ovom
# primeru:
str(titanic)
# Da obezbedimo reproduktibilnost
set.seed(333)
# Za potrebe ovog primera cemo na osnovu "titanic" data set-a formirati
# "training" i "test" set podataka koje cemo, respektivno, koristiti za trening naseg
# klasifikacionog algoritma (Decision Tree) i njegovo testiranje
#
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka, tj. vrsimo trening naseg modela:
tree <- rpart(survived ~ ., data = train, method = "class")
# Hajde da vidimo kako nase stablo odlucivanja izgleda
fancyRpartPlot(tree)
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
str(titanic)
train_ind=sample(1:nrow(titanic), round(0.75*(nrow(titanic))))
train_ind
str(train_ind)
668/891
train <- titanic[train_ind]
train_ind=sample(1:nrow(titanic), round(0.75*(nrow(titanic))))
train <- titanic[train_ind]
train_ind <- sample(1:nrow(titanic), round(0.75*(nrow(titanic))))
train <- titanic[train_ind, ]
test <- titanic[-train_ind, ]
str(train)
str(test)
str(train)
str(test)
tree <- rpart(survived ~ ., data = train, method = "class")
fancyRpartPlot(tree)
# Ucitajmo prvo pakete koji ce biti korisceni u ovom primeru
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Da se podsetimo kako izgleda skup podataka koji cemo koristiti u ovom
# primeru:
str(titanic)
# Da obezbedimo reproduktibilnost
set.seed(333)
# Za potrebe ovog primera cemo na osnovu "titanic" data set-a formirati
# "train" i "test" set podataka koje cemo, respektivno, koristiti za trening naseg
# klasifikacionog algoritma (Decision Tree) i njegovo testiranje.
# Podelu cemo izvrsiti tako da "train" set sadrzi 75% podataka,
# a "test" preostalih 25%.
train_ind <- sample(1:nrow(titanic), round(0.75*(nrow(titanic))))
train <- titanic[train_ind, ]
test <- titanic[-train_ind, ]
# Proverimo da li "train" i "test" data set izgledaju kako bi trebalo
str(train)
str(test)
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka, tj. vrsimo trening naseg modela:
tree <- rpart(survived ~ ., data = train, method = "class")
# Hajde da vidimo kako nase stablo odlucivanja izgleda
fancyRpartPlot(tree)
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = titanic, type = "class")
fancyRpartPlot(tree)
fancyRpartPlot(tree)
# Ucitajmo prvo pakete koji ce biti korisceni u ovom primeru
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Da se podsetimo kako izgleda skup podataka koji cemo koristiti u ovom
# primeru:
str(titanic)
# Da obezbedimo reproduktibilnost
set.seed(333)
# Za potrebe ovog primera cemo na osnovu "titanic" data set-a formirati
# "train" i "test" set podataka koje cemo, respektivno, koristiti za trening naseg
# klasifikacionog algoritma (Decision Tree) i njegovo testiranje.
# Podelu cemo izvrsiti tako da "train" set sadrzi 75% podataka,
# a "test" preostalih 25%.
train_ind <- sample(1:nrow(titanic), round(0.75*(nrow(titanic))))
train <- titanic[train_ind, ]
test <- titanic[-train_ind, ]
# Proverimo da li "train" i "test" data set izgledaju kako bi trebalo
str(train)
str(test)
# Generisemo klasifikacioni model (drvo odlucivanja - decision tree) na osnovu datih podataka, tj. vrsimo trening naseg modela:
tree <- rpart(survived ~ ., data = train, method = "class")
# Hajde da vidimo kako nase stablo odlucivanja izgleda
fancyRpartPlot(tree)
# Koristimo predict() funkciju da predvidimo klase
pred <- predict(tree, newdata = test, type = "class")
fancyRpartPlot(tree)
str(train)
head(train)
table(titanic$survived)
prop.table(table(titanic$survived))
prop.table(table(train$survived))
prop.table(table(train$survived))
fancyRpartPlot(tree)
plot(tree)
prp(tree)
?prp
?rpart
pred <- predict(tree, newdata = test, type = "class")
pred <- predict(tree, newdata = test, type = "class")
table(pred$survived)
table(pred)
prop.table(table(pred))
conf <- table(test$Survived, pred)
str(test)
conf <- table(test$survived, pred)
sum(diag(conf))/sum(conf)
